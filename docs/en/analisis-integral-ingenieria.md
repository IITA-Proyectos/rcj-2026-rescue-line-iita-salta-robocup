<!-- AUTO-GENERATED FILE – DO NOT EDIT.
     Source: docs/es (Spanish is source of truth)
     This file is generated by GitHub Actions.
-->

# Integral Engineering Analysis — IITA Salta RCJ 2026 Rescue Line

**Repository:** `IITA-Proyectos/rcj-2026-rescue-line-iita-salta-robocup`  
**Date of analysis:** February 23, 2026  
**Analyst:** Claude (at the request of Gustavo Viollaz, Team Director)

---

## 1. Executive Summary

The IITA Salta team presents a dual-processor robot (Teensy 4.1 + Raspberry Pi 4B) for RoboCup Junior 2026 Rescue Line. The system demonstrates a remarkable level of technical ambition for the category: computer vision with YOLOv8, a custom binary serial protocol, differential kinematics with PID, and a claw mechanism with 5 servos. The team won the Argentine national championship in 2025, validating the work foundation.

However, the detailed analysis of the code reveals significant structural issues that, if not resolved before Incheon, could compromise the robot's reliability in international competition. The main findings are:

- **The Teensy firmware (main.cpp) is a monolith of ~700 lines without a formal state machine**, with hardcoded rescue logic for fixed times and angles, lacking error recovery.
- **The Raspberry program (Main.py) mixes all logic in a single file of ~500 lines**, with global variables crossed between states and no robust exception handling.
- **The serial protocol lacks checksums and acknowledgments**, making it vulnerable to desynchronization under electrical noise — a common scenario in competition with DC motors.
- **The rescue zone relies on blind timing sequences** (`runTime`) instead of sensor feedback, making it fragile to variations in battery, surface, and friction.
- **There is significant duplicated code** between `Main.py`, `rescatemodelonos.py`, and `tfmodelprueba.py`, creating a risk of divergence.

The robot has a solid foundation. Recommendations are prioritized according to impact on competition and feasibility before Incheon.

---

## 2. System Architecture

### 2.1 Processor Diagram

```
┌─────────────────────┐     UART 115200      ┌──────────────────────┐
│   RASPBERRY PI 4B   │◄────────────────────►│    TEENSY 4.1         │
│   (8 GB RAM)        │   /dev/serial0        │                      │
│                     │   Serial5             │                      │
│  • OpenCV (vision)  │                      │  • 4x DC motors      │
│  • YOLOv8 (ONNX)   │   Protocol:           │  • 5x servos (claw)  │
│  • USB 2MP Camera   │   [255,spd,254,ang,  │  • BNO055 (IMU)      │
│    WIDE 140°        │    253,gs,252,sl]     │  • 2x VL53L0X (ToF)  │
│                     │                      │  • 3x ultrasonic     │
│  States:           │   RPi→Teensy: 8 bytes│  • APDS9960 (color)  │
│  waiting→line→     │   Teensy→RPi: 1 byte │  • Encoders x4       │
│  rescue→deposit     │                      │                      │
└─────────────────────┘                      └──────────────────────┘
```

### 2.2 Architecture Evaluation

**Strengths:**
- The separation of vision (RPi) and control (Teensy) is a wise decision. The Teensy 4.1 at 600 MHz provides control determinism that the RPi cannot guarantee.
- The Raspberry Pi 4B with 8 GB is more than sufficient for YOLOv8 in ONNX.
- The WIDE 140° camera is a good choice for line following and rescue zone.

**Weaknesses:**
- There is no second communication channel (neither I2C nor SPI backup). If the UART fails, the robot becomes completely blind.
- There is no bidirectional watchdog mechanism: if the RPi freezes, the Teensy does not detect it (and vice versa).
- The Teensy lacks persistent logging (neither SD card nor circular buffer), complicating post-competition diagnostics.

---

## 3. Analysis of Teensy Firmware (main.cpp)

### 3.1 General Structure

The `main.cpp` file has approximately 700 lines and concentrates all the robot's logic: hardware initialization, sensor reading, implicit state machine, movement routines, and rescue sequences.

**Fundamental Problem:** There is no formal state machine. The logic is controlled by a `String routine` ("line" or "rescue") and nested `while` loops within `loop()`. This generates:

- Difficulty in adding new states without introducing bugs.
- Inability to make clean transitions between states.
- Code that is difficult to test in isolation.

### 3.2 Critical Findings in the Firmware

**3.2.1 — Use of `String` instead of enums**

```cpp
String routine = "line";
String wall = "right";
String silver_side = "";
String wall = "";
```

In a microcontroller, Arduino's `String` causes heap fragmentation. For a competition program that runs continuously, this can lead to random crashes after minutes of operation. `enum class` or at least `const char*` with `strcmp` comparisons should be used.

**3.2.2 — Global variables without concurrency protection**

```cpp
double speed;
double steer;
int green_state = 0;
int silver_line = 0;
```

These variables are written in `serialEvent5()` (potentially from interrupt or polling) and read in `loop()`. There are no `volatile`, atomic reads, or mutex. In Teensy 4.1, if `serialEvent5()` runs as polling (not ISR), the risk is lower, but the practice is dangerous and should be explicitly documented or corrected.

**3.2.3 — Hardcoded rescue sequences by time**

```cpp
// Example of blind sequence:
runTime(20, FORWARD, 0, 1500);
runTime(0, BACKWARD, 0, 1000);
runAngle(30, FORWARD, 45);
runTime(30, FORWARD, 0, 3000);
```

These sequences assume that the robot moves exactly N milliseconds to cover X distance. In competition, this fails because:
- The battery discharges and speed varies.
- The surface changes (carpet vs. wood vs. vinyl).
- Obstacles and speed bumps alter the trajectory.

The `runDistance()` function exists and uses encoders, but it is underutilized. The recommendation is to migrate all rescue sequences to `runDistance()` + `runAngle()`.

**3.2.4 — The case `action = 1` (front obstacle) uses random**

```cpp
RanNumber = random(1, 3);
if (RanNumber == 1) {
    runAngle(25, FORWARD, -95);
    // ...search for line to the left
}
if (RanNumber == 2) {
    runAngle(25, FORWARD, 95);
    // ...search for line to the right
}
```

Choosing randomly which side to dodge an obstacle is a last-resort strategy. The lateral ToF sensors are already available and should be used to decide which side has more space. This is a simple high-impact change.

**3.2.5 — The `runAngle` function has duplicated and fragile logic**

The function attempts to handle specific angles (180, 90, -90, 45, -45) with special cases, but the general case (`else if (angle > 0)`) always turns at maximum steer=1 or -1, without proportional control. This causes overshooting. A simple P controller (`steer = constrain(Kp * error, -1, 1)`) would be more robust and eliminate all special cases.

**3.2.6 — Inconsistency in servo pins**

In `main.cpp`:
```cpp
DFServo sort(23, ...);
DFServo deposit(12, ...);
```

In `test/actuators/clawLibTest.cpp`:
```cpp
DFServo sort(12, ...);
DFServo deposit(23, ...);
```

The pins for `sort` and `deposit` are inverted between the main program and the test. This indicates that the test does not reflect the actual hardware, invalidating it as a diagnostic tool.

**3.2.7 — The rescue zone exit is not implemented**

The backlog confirms: "Exit from the rescue zone correctly — we have a base but it does not work with obstacles." In the code, `deposit_times == 2` triggers a wall alignment sequence, but there is no logic to find the black tape exit. This is a critical gap for scoring.

### 3.3 Firmware Action Table

| action | Trigger | Behavior | Risk |
|--------|---------|----------|------|
| 1 | Front obstacle (<12 cm) | Random left/right dodge | Does not use lateral ToF |
| 2 | silver_line == 1 | Enters rescue | Blind sequence by time |
| 5 | green_state == 2 | Right turn (green) | Fixed angle 60° |
| 6 | green_state == 1 | Left turn (green) | Fixed angle 60° |
| 7 | green_state == 0 | Line following | Works |
| 12 | green_state == 14 | Alignment and wait RPi | Partial implementation |
| 14 | green_state == 3 | Half turn (double green) | Fixed angle 180° |

---

## 4. Analysis of Raspberry Pi Software (Main.py)

### 4.1 General Structure

`Main.py` is a ~500 line file that implements everything: line following by classic vision, YOLO detection for rescue, state machine, serial communication, and threading. The `modo_rescate()` function is a mega-function of ~300 lines with classes, threads, and all the rescue logic inside.

### 4.2 Critical Findings

**4.2.1 — All logic in a single file**

The file mixes responsibilities that should be separated:
- State machine → own module
- Line vision → own module
- Serial communication → own module
- YOLO + tracking → own module

This makes any change require understanding all 500 lines and prevents two people from working in parallel without merge conflicts.

**4.2.2 — Global variables crossed between states**

```python
estado = 'esperando'  # global
silver_line = False    # global, modified in line, read in rescue
```

The variable `estado` is modified both in the main loop and within `modo_rescate()` and `serial_monitor_local()`. This sharing without locks is a race condition waiting to happen, especially with rescue threads.

**4.2.3 — No exception handling in serial**

```python
ser.write(output)  # without try/except
```

If the Teensy disconnects or the buffer fills, `ser.write()` can throw an exception that crashes the entire program. In competition, this means total robot stop.

**4.2.4 — Hardcoded color thresholds**

```python
lower_black = np.array([0, 0, 0])
upper_black = np.array([90, 90, 90])
lower_green = np.array([120, 90, 100])
upper_green = np.array([170, 120, 140])
```

These values work in a specific lighting environment. In international competition, the lighting will be different. There is no automatic or adaptive calibration system. The backlog confirms this concern.

**4.2.5 — The vision resolution is extremely low**

```python
width, height = 160, 120
```

At 160x120, each pixel covers a significant portion of the visual field. This limits the angular precision of the line follower and the detection of green squares. High-level international teams usually use 320x240 or higher.

**4.2.6 — The line angle calculation is simplistic**

```python
x_resultant = np.mean(x_black)
y_resultant = np.mean(y_black)
angle = (math.atan2(y_resultant, x_resultant) / math.pi * 180) - 90
```

A weighted centroid of all black pixels is calculated, which works for straight lines but has problems at intersections, tight curves, and when there is noise (shadows). Techniques like look-ahead to multiple ROIs, contour segmentation, or curvature analysis are not used.

**4.2.7 — Silver detection is fragile**

```python
lower_silver_hsv = np.array([79, 16, 46])
upper_silver_hsv = np.array([168, 28, 79])
```

The names suggest HSV but the values are applied as BGR (`cv2.inRange(frame_resized, ...)`). This is confusing and potentially incorrect. Additionally, the area threshold for detecting silver is only 50 pixels, leading to false positives.

### 4.3 Line Vision Pipeline — Flow

```
Camera (160x120) → rotate 180° → resize →
  ├── Black mask (BGR) → weighted centroid → angle
  ├── Green mask (LAB) → square detection → green_state
  ├── Red mask (HSV) → red line detection → green_state=10
  └── Silver mask (BGR) → rescue entry detection → silver_line=1
→ Serial: [255, speed, 254, angle+90, 253, green_state, 252, silver_line]
```

### 4.4 YOLO Rescue Pipeline — Flow

```
Camera → Capture thread → Queue →
  Inference thread (YOLO ONNX 256x256) → Queue →
  Main loop:
    ├── Detections → MOSSE tracker / CentroidTracker
    ├── Target selection → simple proportional control
    └── Serial: [255, speed, 254, angle+90, 253, green_state, 252, 0]
```

The multithreaded pipeline is good engineering. The separation of capture/inference/control maximizes throughput.

---

## 5. Communication Protocol

### 5.1 Current Format

**RPi → Teensy (8 bytes):**
```
[255] [speed] [254] [angle] [253] [green_state] [252] [silver_line]
```

**Teensy → RPi (1 byte):**
```
0xF9 (249) = startup ready
0xF8 (248) = enough balls
0xFF (255) = switch off
```

### 5.2 Protocol Vulnerabilities

**No checksum or CRC:** If a byte is corrupted by electromagnetic noise from the motors, the parser desynchronizes. For example, if the marker 255 is lost, the next speed data is interpreted as a marker, and the entire frame shifts. There is no recovery mechanism.

**No acknowledgment:** The RPi sends fire-and-forget commands. If the Teensy is busy in `runTime()` (which is blocking), bytes accumulate in the serial buffer and are processed in batch upon exit, with outdated data.

**Markers are valid data values:** The speed can be 252 or 253, which collide with the markers for green_state and silver_line. In the implementation, `serialEvent5()` assumes that if `data >= 252`, it is a marker. But if the actual speed were 252, it would be interpreted as a silver_line marker. This artificially limits the speed range to 0-251.

**Recommendation:** Add a checksum byte (XOR of the data) and use markers outside the valid data range (or escape the markers with an escape byte).

---

## 6. AI and Vision System

### 6.1 YOLO Model

- Model: `zonasdepositoalta.onnx` (YOLOv8, FP32)
- Runtime: ONNX Runtime on CPU (no accelerator)
- Input: 256x256
- Classes: black (ball), silver (ball), red high (zone), green high (zone)
- Inference: every frame (`DETECT_EVERY = 1`)

**Evaluation:** The decision for FP32 over INT8 is conservative but reasonable given that precision loss was reported in quantization. The input size of 256x256 is a good balance for Raspberry Pi. ONNX Runtime is the best option for ARM without NPU according to the benchmarks the team collected.

### 6.2 Model Concerns

- **No augmentations for variable lighting in the training dataset** (pending in backlog).
- **The model classes in Main.py and rescatemodelonos.py are different:** Main.py uses 4 classes (black, silver, red high, green high) while the test file uses 6 classes (boxgreen, boxred, black, silver, red high, green high). This indicates that the model changed but not all files were updated.
- **No validation that the model loaded correctly.** If the model path is incorrect, the exception is not caught and the robot fails silently.
- **FPS is not documented under competition conditions.** The benchmarks are external. The team needs to measure real FPS with the current model, current camera, and current resolution.

### 6.3 Tracking

MOSSE (OpenCV contrib) is used as the primary tracker and a custom CentroidTracker as a fallback. MOSSE is extremely fast but fragile to sudden scale changes or occlusions. The implemented CentroidTracker is basic but functional.

**Concern:** The function `choose_stable_target()` selects the target closest to the last known, but does not consider the class. If a silver ball passes close to a black one, the tracker may "jump" between targets of different classes.

---

## 7. Custom Libraries (Teensy)

### 7.1 DriveBase

The `DriveBase` class implements differential steering with PID per motor. The PID uses only Ki=22 (without Kp or Kd), which is essentially a pure integral controller. This may work for low speeds but will have overshoot and oscillation problems at high speeds.

The `steer()` function calculates left and right speeds from speed and rotation:
- `rotation = 0`: both sides equal
- `rotation = 1`: left back, right forward (turn in place)
- `rotation = -1`: right back, left forward

### 7.2 Claw

The `Claw` class controls 5 servos: lift, left, right, sort, deposit. The functions `open()`, `close()`, `lift()`, `lower()` move the servos to hardcoded positions. The angles are defined in the .cpp with values like `120`, `90`, `20`, etc. There is no position feedback or jam detection.

---

## 8. Code Quality and Engineering Practices

### 8.1 Positive Aspects

- **Existing and bilingual documentation.** The docs in `docs/es/` are good: serial protocol, libraries, YOLO pipeline. The automated translation workflow is a plus.
- **ICRS repository structure.** It follows a standard with folders for docs, software, hardware, testing, journal.
- **Separate hardware tests.** The `test/` folder has scripts for each sensor and actuator, facilitating diagnostics.
- **Documented backlog.** `pendientes_generales.md` has clear pending items with success criteria.
- **PlatformIO for firmware.** Good choice over Arduino IDE.
- **Requirements.txt for Raspberry.** Dependencies are documented.

### 8.2 Quality Issues

| Problem | Severity | Location |
|---------|----------|----------|
| No type hints in Python | Low | Main.py |
| Commented code not removed (dead code) | Medium | main.cpp, Main.py |
| Debug prints in production | Medium | Main.py (multiple `print()`) |
| No formal logging (only `print()`) | Medium | Both |
| Magic numbers without constants | High | main.cpp (12, 0.7, 55, 3000, etc.) |
| No exception handling in serial | High | Main.py |
| Massive duplication of rescue code | High | Main.py, rescatemodelonos.py, tfmodelprueba.py |
| Shared global variables between threads | High | Main.py |
| Use of `String` in embedded firmware | High | main.cpp |

### 8.3 Git and Workflow

The repo has 4 open issues and 6 open PRs, suggesting active use of GitHub Flow. The CONTRIBUTING.md defines Conventional Commits, branches by type, and declaration of AI usage. The migration from a previous repo is documented.

---

## 9. Competitive Analysis for RoboCup 2026

### 9.1 Capabilities vs. Regulation Requirements

| Requirement | Status | Notes |
|-------------|--------|-------|
| Follow black line | ✅ Functional | Low resolution (160x120) limits precision |
| Detect green squares | ✅ Functional | Color detection in LAB |
| Turn in simple green square | ✅ Functional | Fixed angle 60° |
| Double green (half turn) | ✅ Functional | Uses runAngle(180) |
| Avoid obstacles | ⚠️ Partial | Only front ultrasonic, random dodge |
| Tight curves (135°) | ⚠️ In development | Pending in backlog, re-engagement system designed |
| Inclined slopes | ⚠️ Partial | Adjusts speed by pitch, no traction control |
| Red line (signal) | ✅ Detects | Sends green_state=10, Teensy does not process |
| Entry to rescue (silver) | ✅ Functional | Color + contour detection |
| Detect balls (YOLO) | ✅ Functional | 4 classes, ONNX on CPU |
| Pick up balls | ✅ Functional | Claw with 5 servos |
| Distinguish alive/dead | ✅ Functional | Silver vs. black by vision |
| Deposit in correct zone | ⚠️ Partial | Zone detection works, deposit logic basic |
| Exit rescue zone | ❌ Not functional | Recognized in backlog as unresolved |
| Ignore false victims | ⚠️ Not clear | The model should filter them, but there is no evidence of training with false |

### 9.2 Critical Gaps for International Competition

1. **Exit from the rescue zone.** Without this, the robot loses all "rescue" points if it cannot continue the course.
2. **Robustness against variable lighting.** International teams use automatic White Balance or on-the-fly calibration. Fixed thresholds will fail.
3. **Speed bumps in the rescue zone.** No specific handling.
4. **Flashing lights/LEDs in the rescue zone.** The YOLO model is not trained for this.
5. **Gaps in the line.** No re-engagement logic implemented (only designed in backlog).

---

## 10. Prioritized Recommendations

### Priority 1 — CRITICAL (before Incheon, minimum viable)

**R1. Implement exit from the rescue zone.**  
After depositing, use ultrasonics + IMU for wall-following until finding the black tape (detectable by the APDS9960 color sensor, which is already mounted). This unlocks significant points.

**R2. Add checksum to the serial protocol.**  
A XOR byte at the end of the frame. If it does not match, discard the frame. This takes ~1 hour of implementation and prevents crashes due to desynchronization.

**R3. Replace `String` with `enum` in the firmware.**  
Change `String routine` to `enum class Routine { LINE, RESCUE }` and the same for `wall`, `wall`, `silver_side`. This eliminates the risk of heap fragmentation.

**R4. Add try/except to all serial communication of the RPi.**  
```python
try:
    ser.write(output)
except serial.SerialException:
    # attempt to reconnect or continue without serial
```

**R5. Use the lateral ToF to decide the direction of obstacle dodge** instead of `random()`.

### Priority 2 — IMPORTANT (significant performance improvement)

**R6. Migrate rescue sequences from `runTime()` to `runDistance()` + `runAngle()`.**  
The functions already exist. This makes behavior independent of battery level.

**R7. Implement `runAngle()` with proportional control** instead of binary steer ±1.  
```cpp
float steer = constrain(Kp * error, -1.0, 1.0);
robot.steer(speed, FORWARD, steer);
```

**R8. Add semi-automatic color calibration.**  
At startup, capture N frames of the ground and calculate adaptive black thresholds. This can be done in the first seconds of the "waiting" phase.

**R9. Implement a bidirectional watchdog.**  
The Teensy sends a heartbeat every 500ms. If the RPi does not respond in 2s, the Teensy assumes vision failure and enters degraded mode (wall-following with sensors).

**R10. Increase vision resolution to 320x240** for the line. The processing overhead is ~4x but the RPi 4B with 8GB can handle it, and angular precision improves significantly.

### Priority 3 — RECOMMENDED (quality and maintainability improvement)

**R11. Refactor Main.py into modules.**  
- `state_machine.py` — state transitions  
- `line_follower.py` — line vision  
- `rescue_vision.py` — YOLO + tracking  
- `serial_comm.py` — communication with Teensy  
- `main.py` — orchestration  

**R12. Implement a formal state machine in the firmware.**  
```cpp
enum class State { LINE, RESCUE_ENTER, RESCUE_COLLECT, RESCUE_DEPOSIT, RESCUE_EXIT };
State currentState = State::LINE;
void loop() {
    readSensors();
    switch(currentState) { ... }
    actuate();
}
```

**R13. Unify rescue code.** `rescatemodelonos.py` and `tfmodelprueba.py` should be parameterized variants of the same code, not divergent copies.

**R14. Add logging with timestamps** to a file on the RPi for post-competition analysis.

**R15. Document the PID of the motors.** Ki=22 without Kp or Kd is unusual. If it works, document why. If it has not been tuned, perform formal tuning with step response.

---

## 11. Risk Summary for Incheon

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Serial desynchronization due to motor noise | High | High | R2 (checksum) |
| Crash due to heap fragmentation in Teensy | Medium | High | R3 (eliminate String) |
| Vision failure due to different lighting | High | High | R8 (calibration) |
| Unable to exit rescue zone | Certainty | High | R1 (implement exit) |
| Rescue sequences fail due to low battery | High | Medium | R6 (use encoders) |
| Crash of Main.py due to serial exception | Medium | High | R4 (try/except) |
| YOLO model confused by LED flashlights | Medium | Medium | Augmentations in dataset |
| RPi freezes, Teensy does not detect it | Low | High | R9 (watchdog) |

---

## 12. Conclusion

The IITA Salta team has a robot with solid foundations: good processor architecture, functional custom libraries, vision with deep learning, and above-average documentation for the category. The national championship of 2025 validates that the base system works.

To compete at an international level in Incheon, the focus must be on **reliability, not on new features**. Recommendations R1 to R5 (rescue exit, serial checksum, eliminate String, try/except, and use ToF for obstacles) are limited changes that dramatically reduce the most likely failure modes.

The greatest technical risk is **fragility against competition conditions different from training**: lighting, surfaces, battery level, and electromagnetic noise. Every hour invested in robustness (adaptive calibration, watchdogs, fallbacks) is worth more than an hour invested in new features.

---

*Document generated by Claude (Anthropic) as an engineering analysis of the IITA Salta team's repository for RoboCup Junior 2026 Rescue Line. Requested and supervised by Gustavo Viollaz.*